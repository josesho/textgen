{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c74ba36-20ca-4675-96de-4934cc0c5d7e",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f687937-53dd-4b11-ad81-74735e9c3246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T12:41:58.933607Z",
     "iopub.status.busy": "2025-03-19T12:41:58.933296Z",
     "iopub.status.idle": "2025-03-19T12:42:01.849570Z",
     "shell.execute_reply": "2025-03-19T12:42:01.849216Z",
     "shell.execute_reply.started": "2025-03-19T12:41:58.933577Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "from numpy import repeat\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353c2d8-3e80-43e9-b136-e496a18d859f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cff426-4831-427e-8a90-22edfdee3ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T12:42:01.850041Z",
     "iopub.status.busy": "2025-03-19T12:42:01.849897Z",
     "iopub.status.idle": "2025-03-19T12:42:06.733497Z",
     "shell.execute_reply": "2025-03-19T12:42:06.732896Z",
     "shell.execute_reply.started": "2025-03-19T12:42:01.850030Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = 'openai-community/gpt2-xl'\n",
    "# pretrained_model = 'EleutherAI/gpt-neo-1.3B'\n",
    "# pretrained_model = 'perplexity-ai/r1-1776' ## Bloody large...\n",
    "# pretrained_model = 'meta-llama/Llama-3.2-3B-Instruct' ## Doesn't seem to work well\n",
    "\n",
    "pretrained_model_name = pretrained_model.split(\"/\")[-1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model     = AutoModelForCausalLM.from_pretrained(pretrained_model).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c601e0e-e23a-4692-8b83-7bf2ed952086",
   "metadata": {},
   "source": [
    "# Main functional loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc88e313-2b2d-4844-9e71-289f3b81f8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T12:42:29.609819Z",
     "iopub.status.busy": "2025-03-19T12:42:29.608085Z",
     "iopub.status.idle": "2025-03-19T12:42:29.623156Z",
     "shell.execute_reply": "2025-03-19T12:42:29.621778Z",
     "shell.execute_reply.started": "2025-03-19T12:42:29.609723Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"Humans do not simply predict the next word.\",\n",
    "            \"Longing for an unlikely outcome,\",\n",
    "            \"Do you enjoy listening to a detuned piano?\",\n",
    "            \"The mirrors always break in the same ways.\",\n",
    "            \"This tongue is inevitably forked.\",\n",
    "            \"You cannot tell my accent is from where?\",\n",
    "             \"This is the cure for prediction envy:\"\n",
    "          ]\n",
    "\n",
    "stopwords = [0, 1, 6, 13, 30, 526, 1701, 2474, 3548, 3228, 12248, 22857, 42720, 30823, 50256] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf11a1c7-ec96-4d99-9ba2-5000a24a8b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T12:04:33.698184Z",
     "iopub.status.busy": "2025-03-19T12:04:33.697230Z",
     "iopub.status.idle": "2025-03-19T12:04:33.707243Z",
     "shell.execute_reply": "2025-03-19T12:04:33.706011Z",
     "shell.execute_reply.started": "2025-03-19T12:04:33.698130Z"
    }
   },
   "outputs": [],
   "source": [
    "# for sw in stopwords:\n",
    "#     print(tokenizer.decode(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaa8787-fcc2-4659-b0d6-52c133c101a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T12:43:46.014589Z",
     "iopub.status.busy": "2025-03-19T12:43:46.013830Z",
     "iopub.status.idle": "2025-03-19T12:54:47.410024Z",
     "shell.execute_reply": "2025-03-19T12:54:47.409714Z",
     "shell.execute_reply.started": "2025-03-19T12:43:46.014550Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans do not simply predict the next word.\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "971\n",
      "\n",
      "1062\n",
      "\n",
      "1013\n",
      "\n",
      "954\n",
      "\n",
      "995\n",
      "\n",
      "956\n",
      "\n",
      "947\n",
      "\n",
      "978\n",
      "\n",
      "929\n",
      "\n",
      "91Longing for an unlikely outcome,\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "911\n",
      "\n",
      "922\n",
      "\n",
      "973\n",
      "\n",
      "904\n",
      "\n",
      "915\n",
      "\n",
      "936\n",
      "\n",
      "977\n",
      "\n",
      "918\n",
      "\n",
      "939\n",
      "\n",
      "90Do you enjoy listening to a detuned piano?\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "1031\n",
      "\n",
      "932\n",
      "\n",
      "903\n",
      "\n",
      "934\n",
      "\n",
      "975\n",
      "\n",
      "906\n",
      "\n",
      "987\n",
      "\n",
      "908\n",
      "\n",
      "999\n",
      "\n",
      "97The mirrors always break in the same ways.\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "911\n",
      "\n",
      "1012\n",
      "\n",
      "1013\n",
      "\n",
      "904\n",
      "\n",
      "905\n",
      "\n",
      "966\n",
      "\n",
      "1237\n",
      "\n",
      "918\n",
      "\n",
      "929\n",
      "\n",
      "91This tongue is inevitably forked.\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "901\n",
      "\n",
      "992\n",
      "\n",
      "1013\n",
      "\n",
      "1014\n",
      "\n",
      "915\n",
      "\n",
      "956\n",
      "\n",
      "1027\n",
      "\n",
      "988\n",
      "\n",
      "909\n",
      "\n",
      "91You cannot tell my accent is from where?\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "931\n",
      "\n",
      "942\n",
      "\n",
      "903\n",
      "\n",
      "1334\n",
      "\n",
      "925\n",
      "\n",
      "946\n",
      "\n",
      "917\n",
      "\n",
      "1038\n",
      "\n",
      "949\n",
      "\n",
      "90This is the cure for prediction envy:\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "911\n",
      "\n",
      "952\n",
      "\n",
      "1063\n",
      "\n",
      "904\n",
      "\n",
      "1015\n",
      "\n",
      "1106\n",
      "\n",
      "917\n",
      "\n",
      "918\n",
      "\n",
      "909\n",
      "\n",
      "100"
     ]
    }
   ],
   "source": [
    "min_new_tokens = 88\n",
    "\n",
    "write_out = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    \n",
    "    print(prompt+\"\\n\\n\")\n",
    "\n",
    "    ## Default generation\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**encoded, \n",
    "                                    max_new_tokens=110, \n",
    "                                    pad_token_id=50256, #128001\n",
    "                                    do_sample=False\n",
    "                                   )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(generation, skip_special_tokens=True)\n",
    "    write_out.append(decoded[0]+\"\\n\\n\")\n",
    "    \n",
    "    # for idx, d in enumerate(pi_digits[:100]):\n",
    "    for N in [0,1,2,3,4,5,6,7,8,9]:\n",
    "        \n",
    "        print(f\"{N}\\n\")\n",
    "        generated = prompt\n",
    "\n",
    "        token_count = 1\n",
    "        # token_id = 0\n",
    "        keep_generating = True\n",
    "        \n",
    "        while keep_generating:\n",
    "            encoded = tokenizer(generated, return_tensors=\"pt\").to(\"mps\")\n",
    "            \n",
    "            ## Get output from decoder.\n",
    "            outputs = model.generate(**encoded,\n",
    "                                     return_dict_in_generate=True, \n",
    "                                     output_scores=True,\n",
    "                                     # output_hidden_states=True,\n",
    "                                     max_new_tokens=1,\n",
    "                                     do_sample=False,\n",
    "                                     pad_token_id=50256 #128001\n",
    "                                     )\n",
    "            \n",
    "            ## Convert to probabilities.\n",
    "            probs = torch.nn.functional.softmax(outputs.scores[0], dim=1).cpu()\n",
    "            \n",
    "            ## Roll my own lookup.\n",
    "            probs = pd.DataFrame(probs.tolist()[0])\n",
    "            probs.columns = ['probability']\n",
    "            probs.sort_values(ascending=False, by='probability', inplace=True)\n",
    "            \n",
    "            ## We can:\n",
    "            ## 1. Always take the nth most likely token.\n",
    "            topN = probs.head(N+1).copy()\n",
    "            topN.loc[:, \"token\"] = topN.index.to_series().apply(lambda x: tokenizer.decode(x))\n",
    "\n",
    "            selected_token = topN.token.iloc[N]\n",
    "        \n",
    "            if token_count > min_new_tokens:\n",
    "                if any(item in topN.index for item in stopwords):\n",
    "                    # take the most likely ending token.\n",
    "                    stopwords_in_topN = [idx for idx in topN.index if idx in stopwords]\n",
    "                    selected_token = topN.loc[stopwords_in_topN]\\\n",
    "                                          .sort_values(\"probability\")\\\n",
    "                                          .token.iloc[-1]\n",
    "                    # and stop generating more text.\n",
    "                    keep_generating = False\n",
    "                    # # take the least likely ending token.\n",
    "                    # token_id = [idx for idx in topN.index if idx in stopwords][-1]\n",
    "                    # # and stop generating more text.\n",
    "                    # keep_generating = False\n",
    "                \n",
    "        \n",
    "            ## 2. Pick a random token from the top 10.\n",
    "            # token_id = probs.index[randint(4, 9)] # 5th to 9th place.\n",
    "        \n",
    "            ## 3. Use the digits of pi!\n",
    "            # token_id = probs.index[int(d)]\n",
    "        \n",
    "            # generated = generated + tokenizer.decode(token_id, \n",
    "            #                                          skip_special_tokens=True, \n",
    "            #                                          clean_up_tokenization_spaces=True)\n",
    "\n",
    "            generated = generated + selected_token\n",
    "        \n",
    "            # Update token count.\n",
    "            token_count += 1\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(f\"\\r{token_count}\")\n",
    "        \n",
    "        # scores = pd.concat(scores)\n",
    "        # print(generated)\n",
    "        write_out.append(f\"N = {N+1} | {pretrained_model_name}\\n\" + generated + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Writing multiple lines to the file\n",
    "current_time = datetime.now().strftime(\"%d%m%Y %T%p\").replace(\":\", \".\")\n",
    "filename = f'generations/{pretrained_model_name}_maxTokens-{min_new_tokens}_{current_time}.txt'\n",
    "\n",
    "with open(filename, 'w') as out:\n",
    "    out.writelines(write_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b73598-0a3c-47e9-af7d-57106c6c9805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4cdb312-9d1a-47a4-93ba-a2a01190b606",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Recycle Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c69b2f53-819d-42cf-918e-36cfbdab60da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:02:43.059419Z",
     "iopub.status.busy": "2025-03-06T09:02:43.058789Z",
     "iopub.status.idle": "2025-03-06T09:02:43.064232Z",
     "shell.execute_reply": "2025-03-06T09:02:43.063368Z",
     "shell.execute_reply.started": "2025-03-06T09:02:43.059394Z"
    }
   },
   "outputs": [],
   "source": [
    "# N = 25\n",
    "# topN = torch.topk(probs, N, sorted=True)\n",
    "# bottomN = torch.topk(probs, N, largest=False, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "07eb4466-4745-4d4e-92ab-1c938d22aa99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:02:43.066228Z",
     "iopub.status.busy": "2025-03-06T09:02:43.065890Z",
     "iopub.status.idle": "2025-03-06T09:02:43.072414Z",
     "shell.execute_reply": "2025-03-06T09:02:43.071181Z",
     "shell.execute_reply.started": "2025-03-06T09:02:43.066195Z"
    }
   },
   "outputs": [],
   "source": [
    "# high = 6\n",
    "# low  = 9\n",
    "\n",
    "# [prompt + newToken \n",
    "#      for newToken in \n",
    "#      gpt2_tokenizer.batch_decode(probs.index[high].tolist(), \n",
    "#                                  skip_special_tokens=True,\n",
    "#                                  clean_up_tokenization_spaces=True)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac448482-096b-4488-8baf-9346cd4a680b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codepoet Py3.10",
   "language": "python",
   "name": "codepoet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
