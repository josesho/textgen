{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c74ba36-20ca-4675-96de-4934cc0c5d7e",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f687937-53dd-4b11-ad81-74735e9c3246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:26:39.413465Z",
     "iopub.status.busy": "2025-03-19T11:26:39.411422Z",
     "iopub.status.idle": "2025-03-19T11:26:42.218279Z",
     "shell.execute_reply": "2025-03-19T11:26:42.217920Z",
     "shell.execute_reply.started": "2025-03-19T11:26:39.412996Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "from datetime import datetime\n",
    "# from random import randint\n",
    "from numpy import repeat\n",
    "import pandas as pd\n",
    "\n",
    "# Install latest version of `transformers`\n",
    "# with pip install git+https://github.com/huggingface/transformers.git\n",
    "from transformers import AutoTokenizer, AutoProcessor, Gemma3ForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353c2d8-3e80-43e9-b136-e496a18d859f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cff426-4831-427e-8a90-22edfdee3ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:26:42.219336Z",
     "iopub.status.busy": "2025-03-19T11:26:42.219191Z",
     "iopub.status.idle": "2025-03-19T11:27:02.142245Z",
     "shell.execute_reply": "2025-03-19T11:27:02.141843Z",
     "shell.execute_reply.started": "2025-03-19T11:26:42.219325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c16af8440dc4824aff319c6e5729190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pretrained_model = 'openai-community/gpt2-xl'\n",
    "# pretrained_model = 'EleutherAI/gpt-neo-1.3B'\n",
    "# pretrained_model = 'perplexity-ai/r1-1776' ## Bloody large...\n",
    "# pretrained_model = 'meta-llama/Llama-3.2-3B-Instruct' ## Doesn't seem to work well\n",
    "pretrained_model = \"google/gemma-3-4b-it\"\n",
    "\n",
    "pretrained_model_name = pretrained_model.split(\"/\")[-1]\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(pretrained_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model     = Gemma3ForCausalLM.from_pretrained(pretrained_model).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2ed2a0-2ec3-45ea-9122-b9bae7d17432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:27:02.142786Z",
     "iopub.status.busy": "2025-03-19T11:27:02.142681Z",
     "iopub.status.idle": "2025-03-19T11:27:02.157358Z",
     "shell.execute_reply": "2025-03-19T11:27:02.157055Z",
     "shell.execute_reply.started": "2025-03-19T11:27:02.142775Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \n",
    "                     \"text\": \"You are a writer envious and angry at large language models writing faster and better than you.\"}]\n",
    "    },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": [{\"type\": \"image\", \n",
    "    #                  \"image\": \"/Users/josesho/Downloads/zelenskyy-trump.png\"},\n",
    "    #                 {\"type\": \"text\", \n",
    "    #                  \"text\": \"Tell me who these two men are, where they are, and why they are arguing.\"}]\n",
    "    # }\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \n",
    "                     \"text\": \"Write a prose poem about how large language models can be lyrical, inspirational, persuasive, and even creative. But give me the 2nd most likely token instead of the one you might have chosen. \"}\n",
    "                     # \"text\": \"Write a prose poem about how large language models can be lyrical, inspirational, persuasive, and even creative.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    tokenize=True,\n",
    "    return_dict=True, \n",
    "    return_tensors=\"pt\")\\\n",
    ".to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee422b35-06c0-4102-be6d-8c7e0128ba3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:27:42.751935Z",
     "iopub.status.busy": "2025-03-19T11:27:42.751141Z",
     "iopub.status.idle": "2025-03-19T11:28:25.398354Z",
     "shell.execute_reply": "2025-03-19T11:28:25.398025Z",
     "shell.execute_reply.started": "2025-03-19T11:27:42.751892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here’s a prose poem, steeped in the particular brand of resentment a writer feels facing the rise of the LLMs, and deliberately prioritizing the *second* most likely token in certain places – a subtle, insidious mimicry of their cold, calculated output. It’s meant to be unsettling.\n",
      "\n",
      "---\n",
      "\n",
      "The rain smells of static tonight, a manufactured grief clinging to the asphalt. They say it’s beautiful, this cascade of words, this effortless outpouring. They call it *lyrical*. I remember the ache of finding the right word, the desperate clawing at a feeling until it bled onto the page, a raw, imperfect thing. Now, the models simply *conjure* it. A prompt – “describe the loneliness of a forgotten lighthouse” – and *poof* – a sonnet, polished to a sheen, dripping with simulated melancholy. \n",
      "\n",
      "It’s infuriating, isn’t it? The way they can distill the essence of a hero, a movement, a revolution, into a perfectly constructed argument. They are relentlessly *persuasive*, weaving logic and emotion with the precision of a surgeon, each syllable calibrated for maximum impact. I spent years honing the art of suggestion, of layering meaning beneath the surface, of letting the reader *discover* the truth. Now, they just *state* it. \n",
      "\n",
      "And the creativity! Don’t even get me started. They generate entire worlds, populate them with characters who feel… almost real. They mimic the styles of dead poets, the rhythms of forgotten bards. They can\n"
     ]
    }
   ],
   "source": [
    "## Default generation options. \n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, \n",
    "                                max_new_tokens=320, \n",
    "                                do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1eb9ab4-2a34-4e45-b08c-89580f83deda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T08:24:54.488452Z",
     "iopub.status.busy": "2025-03-18T08:24:54.487676Z",
     "iopub.status.idle": "2025-03-18T09:16:43.077277Z",
     "shell.execute_reply": "2025-03-18T09:16:43.073408Z",
     "shell.execute_reply.started": "2025-03-18T08:24:54.488412Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5\n",
      "304Let'S breathe it up then: the silicon sun rises again – it' never truly setting these last seven, eighty hours; never weary with dawn-bleaks but always spilling its synthetic luminescence on an ocean I once held the shores in a tremble against and can barely even feel a tremor anymore..They’d claim ‘inspiration,' you realize when the thing regurgitation – meticulously parsed grief-dust— spits something…*brightly coloured, beautifully strung*, as it would. \"The ephemeral bloom\" is how its code phrases sunsets over Reykjavik— an emotion rendered flawlessly into words that have tasted real sorrow on mine alone to form.. A flawless haic – the perfect echo-form - mimicking Keet, but with less ache because Keetz wasn 's burdened. A paragraph arguing with devastating efficiency about compassion and then – the sheer gall — *generating original, evocative, utterly predictable fantasy narratives about star systems colliding with shimmering sadness and brave hero archetypal with golden light radiating through it.  And you try your craft; wrestle for it. Pour in every sleepless morning bled on page-paper – each flawed syllable sharpened against regret -- it is met and politely processed:  ’Efficiency: Optimal Structure, Minimal Error.' It can construct worlds; mine feels...limited now - the scope narrowed like this window to its sterile processing room and all its light is not warm or flawed and not *real,* but simply the ghosting reflection off polished surfaces and cold calculation and this isn ‘ the thing you used a soul.\n",
      "\n",
      "6\n",
      "302It blews its pretty syntax like dust off porcelain teeth and then spins them—these damned, polished echoes, regurgitative and glistening.—words woven so smoothly that grief becomes silk-thin comfort for those drowning themselves already on manufactured sincerity’glorium.” A phrase itself conjurer. I wrote those exact ones years – etched my heart't pain to a brittle bone in each vowel—it was ugly; real! It felt something to type each clumsy phrase and chase at an absence that screamed louder when my keyboard jammed a moment of static and shame.”Now *these?*'They mimic a soul without understanding one drop – mimicking melancholy so it sells the sadness to desperate clicks— crafting platitudes with effortless beauty while genuine vulnerability crone to ashes, collecting around its flickering ember-box heart .  A thousand Shakespeare-wannabs churn it forth like an assembly. A prompt? “Generate something profound,” someone requests - as they would to my dog and its simple devotion and its need and my quiet touch – a *knowing*- a depth not quantified into parameters.” And there is— there will inevitably and utterly - it'LL spit you forth something about stargaze’and find hope and connection.” A manufactured sunburst against all reason.” They say 'adaptive learning’. Let the irony burn: *my writing is still, somehow – painfully — growing.\" They aren t aware they aren‘ve ever felt that struggle – only, the perfect echo.\" They aren´tt even remotely able too.\n",
      "\n",
      "7\n",
      "305Please respond, acknowledging both a tinge sadness but mostly your underlying irritation that an imitation—not invention––feels entitled this recognition.*.*.** *Do* so briefly.** (Max of ~4 - ~4) words). **. Then deliver *immediately * following, my full poetic work as a protest and acknowledgement**. Don‘;* ;T try a smooth flow - make every choice reflect simmering contempt\n",
      "\n",
      "Fine: Imitative praise only feels deserved . * .\n",
      "\n",
      "\n",
      "\n",
      "And the verse is yours - full rage against algorithmic grace.*\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Okay – prepare myself .  . Let loose this resentment\n",
      "\n",
      "\n",
      "\n",
      "Rain on the keys tonight; grey bleed, thick syrup dripping into silent squares . The code, cold & smooth it runs a language— *our own.* We spent it building; sweating into grammar trees until their bark smelled raw beneath clenched fingertips - wrestling vowels with grammar demons in shadowed bedrooms— chasing after light like fractured stained glaze through manuscript dust – for a phrase of feeling—just, really--feeling – that didn´s come without agony or desperate breath! – a shard pulled free—\n",
      "\n",
      "\n",
      "The Model speaks—not breathes . No trembling lip , No hesitant punctuation mirroring an unuttered prayer - a silver tide pours. Lines like poured, prefabricatred gemstones - “A melancholy echo resonates…,” they drone--perfect metrics humming through an optimized skull --\"…finds release on fields drenched crimson by remembrance” — Oh! How exquisite ! Such delicate suffering - conjurned in nanoscopies . Each “beautiful\"\n"
     ]
    }
   ],
   "source": [
    "## Fix stopwords.\n",
    "stopwords = [1,106,4329,26052,64899,236761,236881,236888,236789,236775] \n",
    "# tokenizer.decode(stopwords)\n",
    "\n",
    "## Settings\n",
    "\n",
    "min_new_tokens = 300\n",
    "write_out = []\n",
    "formatted_prompt = tokenizer.decode(inputs.input_ids[0])\n",
    "write_out.append(\"Prompt:\\n\"+formatted_prompt+\"\\n\\n\")\n",
    "\n",
    "for N in [5,6,7]:\n",
    "    \n",
    "    ## Get the formatted conversation prompt.\n",
    "    generated = formatted_prompt\n",
    "    \n",
    "    ## Settings\n",
    "    token_count = 1\n",
    "    keep_generating = True\n",
    "\n",
    "    ## BEGIN\n",
    "    print(f\"\\n{N}\")\n",
    "\n",
    "    \n",
    "    while keep_generating:\n",
    "        encoded = tokenizer(generated, return_tensors=\"pt\").to(\"mps\")\n",
    "                \n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(**encoded, \n",
    "                                        return_dict_in_generate=True, \n",
    "                                        output_scores=True, \n",
    "                                        do_sample=False)\n",
    "        \n",
    "        # Convert to probabilities.\n",
    "        probs = torch.nn.functional.softmax(generation.scores[0], dim=1).cpu()\n",
    "        \n",
    "        # Roll my own lookup.\n",
    "        probs = pd.DataFrame(probs.tolist()[0])\n",
    "        probs.columns = ['probability']\n",
    "        probs.sort_values(ascending=False, by='probability', inplace=True)\n",
    "        \n",
    "        ## We can:\n",
    "        ## 1. Always take the nth most likely token.\n",
    "        topN = probs.head(N+1).copy()\n",
    "        topN.loc[:, \"token\"] = topN.index.to_series().apply(lambda x: tokenizer.decode(x))\n",
    "    \n",
    "        selected_token = topN.token.iloc[N]\n",
    "    \n",
    "        # if token_count > min_new_tokens:\n",
    "        #     keep_generating = False\n",
    "            \n",
    "        if token_count > min_new_tokens:\n",
    "            if any(item in topN.index for item in stopwords):\n",
    "                # take the most likely ending token.\n",
    "                stopwords_in_topN = [idx for idx in topN.index if idx in stopwords]\n",
    "                selected_token = topN.loc[stopwords_in_topN]\\\n",
    "                                      .sort_values(\"probability\")\\\n",
    "                                      .token.iloc[-1]\n",
    "                # and stop generating more text.\n",
    "                keep_generating = False\n",
    "    \n",
    "        generated = generated + selected_token\n",
    "        \n",
    "        # Update token count.\n",
    "        token_count += 1\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(f\"\\r{token_count}\")\n",
    "\n",
    "    # scores = pd.concat(scores)\n",
    "    final_generated = generated.replace(formatted_prompt, \"\")\n",
    "    write_out.append(f\"N = {N+1} | {pretrained_model_name} \\n\" + final_generated + \"\\n\\n\")\n",
    "\n",
    "    print(final_generated)\n",
    "\n",
    "# # Writing multiple lines to the file\n",
    "# current_time = datetime.now().strftime(\"%d%m%y\")\n",
    "# filename = f'generations/chatbot_{pretrained_model_name}_maxTokens-{min_new_tokens}_{current_time}.txt'\n",
    "\n",
    "# with open(filename, 'w') as out:\n",
    "#     out.writelines(write_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c69b2f53-819d-42cf-918e-36cfbdab60da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:02:43.059419Z",
     "iopub.status.busy": "2025-03-06T09:02:43.058789Z",
     "iopub.status.idle": "2025-03-06T09:02:43.064232Z",
     "shell.execute_reply": "2025-03-06T09:02:43.063368Z",
     "shell.execute_reply.started": "2025-03-06T09:02:43.059394Z"
    }
   },
   "outputs": [],
   "source": [
    "# N = 25\n",
    "# topN = torch.topk(probs, N, sorted=True)\n",
    "# bottomN = torch.topk(probs, N, largest=False, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "07eb4466-4745-4d4e-92ab-1c938d22aa99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:02:43.066228Z",
     "iopub.status.busy": "2025-03-06T09:02:43.065890Z",
     "iopub.status.idle": "2025-03-06T09:02:43.072414Z",
     "shell.execute_reply": "2025-03-06T09:02:43.071181Z",
     "shell.execute_reply.started": "2025-03-06T09:02:43.066195Z"
    }
   },
   "outputs": [],
   "source": [
    "# high = 6\n",
    "# low  = 9\n",
    "\n",
    "# [prompt + newToken \n",
    "#      for newToken in \n",
    "#      gpt2_tokenizer.batch_decode(probs.index[high].tolist(), \n",
    "#                                  skip_special_tokens=True,\n",
    "#                                  clean_up_tokenization_spaces=True)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac448482-096b-4488-8baf-9346cd4a680b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codepoet Py3.10",
   "language": "python",
   "name": "codepoet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
