{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c74ba36-20ca-4675-96de-4934cc0c5d7e",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f687937-53dd-4b11-ad81-74735e9c3246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:14:54.848952Z",
     "iopub.status.busy": "2025-03-12T09:14:54.847145Z",
     "iopub.status.idle": "2025-03-12T09:14:57.341920Z",
     "shell.execute_reply": "2025-03-12T09:14:57.341577Z",
     "shell.execute_reply.started": "2025-03-12T09:14:54.848566Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "from datetime import datetime\n",
    "# from random import randint\n",
    "from numpy import repeat\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353c2d8-3e80-43e9-b136-e496a18d859f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cff426-4831-427e-8a90-22edfdee3ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:14:57.342810Z",
     "iopub.status.busy": "2025-03-12T09:14:57.342664Z",
     "iopub.status.idle": "2025-03-12T09:15:02.321226Z",
     "shell.execute_reply": "2025-03-12T09:15:02.320617Z",
     "shell.execute_reply.started": "2025-03-12T09:14:57.342800Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = 'openai-community/gpt2-xl'\n",
    "# pretrained_model = 'EleutherAI/gpt-neo-1.3B'\n",
    "# pretrained_model = 'perplexity-ai/r1-1776' ## Bloody large...\n",
    "# pretrained_model = 'meta-llama/Llama-3.2-3B-Instruct' ## Doesn't seem to work well\n",
    "\n",
    "pretrained_model_name = pretrained_model.split(\"/\")[-1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model     = AutoModelForCausalLM.from_pretrained(pretrained_model).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c601e0e-e23a-4692-8b83-7bf2ed952086",
   "metadata": {},
   "source": [
    "# Main functional loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245055a1-4fba-4980-96df-4172568a2993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:02.322146Z",
     "iopub.status.busy": "2025-03-12T09:15:02.321849Z",
     "iopub.status.idle": "2025-03-12T09:15:02.326692Z",
     "shell.execute_reply": "2025-03-12T09:15:02.326378Z",
     "shell.execute_reply.started": "2025-03-12T09:15:02.322131Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = [0, 1, 6, 13, 30, 3548, 3228, 50256] \n",
    "# double quotation mark, single quotation mark,\n",
    "# exclamation mark, fullstop, \n",
    "# question mark, two question marks, \n",
    "# two exclamation marks, EOS.\n",
    "\n",
    "prompts = [\"The model exhibits signs of prediction envy\",\n",
    "           \"Humans do not simply predict the next word.\",\n",
    "           \"Longing for an unlikely outcome,\",\n",
    "           \"Do you enjoy listening to a detuned piano?\",\n",
    "           \"The mirrors always break in the same ways.\",\n",
    "           \"This tongue is inevitably forked.\",\n",
    "           \"This is the cure for prediction envy:\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75a6900-2cc2-4481-837b-ddb71a575601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:15:02.327159Z",
     "iopub.status.busy": "2025-03-12T09:15:02.327075Z",
     "iopub.status.idle": "2025-03-12T09:16:08.305867Z",
     "shell.execute_reply": "2025-03-12T09:16:08.305529Z",
     "shell.execute_reply.started": "2025-03-12T09:15:02.327150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model exhibits signs of prediction envy\n",
      "95Humans do not simply predict the next word.\n",
      "90Longing for an unlikely outcome,\n",
      "90Do you enjoy listening to a detuned piano?\n",
      "92The mirrors always break in the same ways.\n",
      "96This tongue is inevitably forked.\n",
      "90This is the cure for prediction envy:\n",
      "92"
     ]
    }
   ],
   "source": [
    "min_tokens = 88\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    generated = prompt\n",
    "    keep_generating = True\n",
    "    \n",
    "    for idx, d in enumerate(digits.PHI):\n",
    "    \n",
    "        if keep_generating:\n",
    "            encoded = tokenizer(generated, return_tensors=\"pt\").to(\"mps\")\n",
    "            \n",
    "            # Get output from decoder.\n",
    "            outputs = model.generate(**encoded,\n",
    "                                     return_dict_in_generate=True, \n",
    "                                     output_scores=True,\n",
    "                                     # output_hidden_states=True,\n",
    "                                     max_new_tokens=1,\n",
    "                                     do_sample=False,\n",
    "                                     pad_token_id=50256 #128001\n",
    "                                     )\n",
    "            \n",
    "            # Convert to probabilities.\n",
    "            probs = torch.nn.functional.softmax(outputs.scores[0], dim=1).cpu()\n",
    "    \n",
    "            # Roll my own lookup.\n",
    "            probs = pd.DataFrame(probs.tolist()[0])\n",
    "            probs.columns = ['probability']\n",
    "            probs.sort_values(ascending=False, by='probability', inplace=True)\n",
    "            \n",
    "            # Use the digits of pi!\n",
    "            token_id = probs.index[int(d)]\n",
    "            top10 = probs.head(10).copy()\n",
    "        \n",
    "            if idx > min_tokens-1:\n",
    "                if any(item in top10.index for item in stopwords):\n",
    "                    # take the least likely ending token.\n",
    "                    token_id = [i for i in top10.index if i in stopwords][-1]\n",
    "                    # and stop generating more text.\n",
    "                    keep_generating = False\n",
    "                    \n",
    "            generated = generated + tokenizer.decode(token_id, \n",
    "                                                     skip_special_tokens=True, \n",
    "                                                     clean_up_tokenization_spaces=True)\n",
    "        \n",
    "            # Update current token count.\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(f\"\\r{idx+1}\")\n",
    "    \n",
    "    # Writing to file.\n",
    "    current_time = datetime.now().strftime(\"%d%m%y\")\n",
    "    filename = f'generations/Phi_GoldenRatio_Digits_{prompt}_{pretrained_model_name}_maxTokens-{min_tokens}_{current_time}.txt'\n",
    "    \n",
    "    with open(filename, 'w') as out:\n",
    "        out.write(f\"{pretrained_model_name} \\n\" + generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac448482-096b-4488-8baf-9346cd4a680b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codepoet",
   "language": "python",
   "name": "codepoet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
